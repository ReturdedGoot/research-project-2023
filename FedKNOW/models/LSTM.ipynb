{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch import optim\n",
    "\n",
    "\n",
    "# Here is the parts for the dataloader\n",
    "batchsize = 100\n",
    "training_data = datasets.FashionMNIST(root=\"../fashion_mnist\", train=True, transform=transforms.ToTensor(), download=True)\n",
    "testing_data = datasets.FashionMNIST(root=\"../fashion_mnist\", train=False, transform=transforms.ToTensor(), download=True)\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(training_data, batch_size=batchsize)\n",
    "test_dataloader = DataLoader(testing_data, batch_size=batchsize)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Hyperparameters\n",
    "sequence_length = 28\n",
    "input_length = 28\n",
    "hidden_size = 128 #this is the number of LSTM cells in each layer\n",
    "num_layers = 2 #Number of LSTM layers in total.\n",
    "num_classes = 10\n",
    "num_epochs = 10\n",
    "learning_rate = 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Defining the LSTM pytorch implementation\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_length, hidden_size, num_layers, num_classes):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        #storing hyperparameter\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.num_classes = num_classes\n",
    "        self.input_length = input_length\n",
    "\n",
    "        #The neural network.\n",
    "        self.lstm = nn.LSTM(input_size=input_length, hidden_size=hidden_size, num_layers=num_layers, batch_first = True).to('cuda')\n",
    "        self.output_layer = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, input):\n",
    "        hidden_states = torch.zeros(self.num_layers, input.size(0), self.hidden_size)\n",
    "        hidden_states = hidden_states.to('cuda')\n",
    "        cell_states = torch.zeros(self.num_layers, input.size(0), self.hidden_size)\n",
    "        cell_states = cell_states.to('cuda')\n",
    "\n",
    "        out, _ = self.lstm(input, (hidden_states, cell_states))\n",
    "        out = self.output_layer(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LSTM(\n",
      "  (lstm): LSTM(28, 128, num_layers=2, batch_first=True)\n",
      "  (output_layer): Linear(in_features=128, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "## setting the model\n",
    "model = LSTM(input_length, hidden_size, num_layers, num_classes)\n",
    "model = model.to('cuda')\n",
    "print(model)\n",
    "\n",
    "# defining loss and optimization functions\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, batch: 100/600, loss: 0.894352\n",
      "epoch: 1, batch: 200/600, loss: 0.663611\n",
      "epoch: 1, batch: 300/600, loss: 0.502366\n",
      "epoch: 1, batch: 400/600, loss: 0.615144\n",
      "epoch: 1, batch: 500/600, loss: 0.663134\n",
      "epoch: 1, batch: 600/600, loss: 0.398422\n",
      "epoch: 2, batch: 100/600, loss: 0.430365\n",
      "epoch: 2, batch: 200/600, loss: 0.345645\n",
      "epoch: 2, batch: 300/600, loss: 0.384777\n",
      "epoch: 2, batch: 400/600, loss: 0.408946\n",
      "epoch: 2, batch: 500/600, loss: 0.500300\n",
      "epoch: 2, batch: 600/600, loss: 0.255813\n",
      "epoch: 3, batch: 100/600, loss: 0.301776\n",
      "epoch: 3, batch: 200/600, loss: 0.312696\n",
      "epoch: 3, batch: 300/600, loss: 0.300857\n",
      "epoch: 3, batch: 400/600, loss: 0.354687\n",
      "epoch: 3, batch: 500/600, loss: 0.469427\n",
      "epoch: 3, batch: 600/600, loss: 0.218314\n",
      "epoch: 4, batch: 100/600, loss: 0.375630\n",
      "epoch: 4, batch: 200/600, loss: 0.310765\n",
      "epoch: 4, batch: 300/600, loss: 0.265705\n",
      "epoch: 4, batch: 400/600, loss: 0.277101\n",
      "epoch: 4, batch: 500/600, loss: 0.406030\n",
      "epoch: 4, batch: 600/600, loss: 0.203700\n",
      "epoch: 5, batch: 100/600, loss: 0.277275\n",
      "epoch: 5, batch: 200/600, loss: 0.276958\n",
      "epoch: 5, batch: 300/600, loss: 0.256937\n",
      "epoch: 5, batch: 400/600, loss: 0.340474\n",
      "epoch: 5, batch: 500/600, loss: 0.375170\n",
      "epoch: 5, batch: 600/600, loss: 0.179861\n",
      "epoch: 6, batch: 100/600, loss: 0.280819\n",
      "epoch: 6, batch: 200/600, loss: 0.313675\n",
      "epoch: 6, batch: 300/600, loss: 0.218106\n",
      "epoch: 6, batch: 400/600, loss: 0.300381\n",
      "epoch: 6, batch: 500/600, loss: 0.367397\n",
      "epoch: 6, batch: 600/600, loss: 0.199802\n",
      "epoch: 7, batch: 100/600, loss: 0.191743\n",
      "epoch: 7, batch: 200/600, loss: 0.292708\n",
      "epoch: 7, batch: 300/600, loss: 0.196681\n",
      "epoch: 7, batch: 400/600, loss: 0.303048\n",
      "epoch: 7, batch: 500/600, loss: 0.407630\n",
      "epoch: 7, batch: 600/600, loss: 0.178100\n",
      "epoch: 8, batch: 100/600, loss: 0.275525\n",
      "epoch: 8, batch: 200/600, loss: 0.344384\n",
      "epoch: 8, batch: 300/600, loss: 0.229606\n",
      "epoch: 8, batch: 400/600, loss: 0.275805\n",
      "epoch: 8, batch: 500/600, loss: 0.344753\n",
      "epoch: 8, batch: 600/600, loss: 0.173029\n",
      "epoch: 9, batch: 100/600, loss: 0.181440\n",
      "epoch: 9, batch: 200/600, loss: 0.349914\n",
      "epoch: 9, batch: 300/600, loss: 0.234463\n",
      "epoch: 9, batch: 400/600, loss: 0.210497\n",
      "epoch: 9, batch: 500/600, loss: 0.286849\n",
      "epoch: 9, batch: 600/600, loss: 0.182216\n",
      "epoch: 10, batch: 100/600, loss: 0.206866\n",
      "epoch: 10, batch: 200/600, loss: 0.276896\n",
      "epoch: 10, batch: 300/600, loss: 0.234171\n",
      "epoch: 10, batch: 400/600, loss: 0.294736\n",
      "epoch: 10, batch: 500/600, loss: 0.416127\n",
      "epoch: 10, batch: 600/600, loss: 0.210511\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Training method\n",
    "## Running the tests\n",
    "\n",
    "def train(num_epochs, model, train_dataloader, loss_func):\n",
    "    total_steps = len(train_dataloader)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch, (image, labels) in enumerate(train_dataloader):\n",
    "            images = image.reshape(-1, sequence_length, input_length)\n",
    "            images = images.to('cuda')\n",
    "            labels = labels.to('cuda')\n",
    "            output = model(images)\n",
    "            loss = loss_func(output,labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            if(batch+1)%100 ==0:\n",
    "                print(f\"epoch: {epoch+1}, batch: {batch+1}/{total_steps}, loss: {loss.item():>4f}\")\n",
    "\n",
    "\n",
    "train(num_epochs, model, train_dataloader, loss_func)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 2, 1, 1, 6, 1, 4, 6, 5, 7, 4, 5, 7, 3, 4, 1, 2, 4, 8, 0, 2, 5, 7, 9,\n",
       "        1, 4, 6, 0, 9, 3, 8, 8, 3, 3, 8, 0, 7, 5, 7, 9, 6, 1, 3, 7, 6, 7, 2, 1,\n",
       "        2, 2, 4, 4, 5, 8, 2, 2, 8, 4, 8, 0, 7, 7, 8, 5, 1, 1, 2, 3, 9, 8, 7, 0,\n",
       "        2, 6, 2, 3, 1, 2, 8, 4, 1, 8, 5, 9, 5, 0, 3, 2, 0, 6, 5, 3, 6, 7, 1, 8,\n",
       "        0, 1, 4, 2])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_images, test_labels = next(iter(test_dataloader))\n",
    "\n",
    "test_images = test_images.to('cuda')\n",
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -4.0447,  -4.0769,  -5.7869,  -6.3180,  -6.2221,   1.5258,  -4.5671,\n",
       "           4.1538,  -2.8187,  10.2990],\n",
       "        [ -0.0414,  -6.9361,   7.5540,  -6.2268,   0.4350,  -6.1794,   2.0721,\n",
       "          -5.5404,  -5.7443,  -6.6442],\n",
       "        [  0.0800,  16.1949,  -2.5659,  -0.3206,   2.8839,  -4.6103,  -1.7340,\n",
       "          -8.0594,  -3.6529,  -4.5061],\n",
       "        [ -0.9564,  16.1161,  -2.9847,   1.7447,   1.9916,  -5.1914,  -1.7775,\n",
       "          -9.5939,  -3.8929,  -5.3975],\n",
       "        [  1.1130,  -7.1868,   0.5320,   0.1699,   1.6714,  -6.4677,   5.2790,\n",
       "          -6.9009,  -3.8740,  -7.0004],\n",
       "        [ -0.7160,  10.8477,  -1.5630,  -1.0714,   0.3839,  -1.0044,  -2.2697,\n",
       "          -4.7273,  -1.1557,  -1.8328],\n",
       "        [ -1.5107,  -4.8968,   4.5134,  -5.6628,   4.9518,  -6.9696,   0.2419,\n",
       "          -6.5179,  -3.8086,  -6.3888],\n",
       "        [ -1.1669,  -7.8321,  -0.3948,  -3.5445,   1.4375,  -6.0183,   8.9003,\n",
       "          -7.5286,  -4.3742,  -7.0778],\n",
       "        [ -2.0071,  -3.8240,  -3.5592,  -3.5480,  -1.8777,   7.4921,  -3.6310,\n",
       "           0.7246,   1.1515,   1.6793],\n",
       "        [ -3.9046,  -6.8838,  -5.1815,  -6.6065,  -6.5456,   0.3324,  -3.4750,\n",
       "           9.4183,  -1.1450,   1.4763],\n",
       "        [ -4.0509,  -7.3149,   2.6801,  -7.7438,   7.8307,  -4.0480,   2.2847,\n",
       "          -7.1538,  -6.0308,  -7.4034],\n",
       "        [ -3.7292,  -3.4853,  -3.4115,  -5.6012,  -1.9640,   8.1551,  -4.6944,\n",
       "           0.4545,  -0.6857,   2.8542],\n",
       "        [ -0.3949,  -7.3678,  -2.1501,  -1.8614,  -4.1019,   2.9312,  -3.6409,\n",
       "           5.0956,   0.7487,  -1.6032],\n",
       "        [ -0.2734,   1.0062,  -3.0063,   8.0173,  -0.8002,  -6.9785,  -1.8316,\n",
       "          -7.8645,  -4.1571,  -7.0243],\n",
       "        [ -2.7108,  -6.5908,   4.2253,  -4.5588,   5.6021,  -5.5513,   0.3180,\n",
       "          -5.3432,  -4.2932,  -5.4355],\n",
       "        [ -1.1299,  15.2611,  -3.0333,   1.9606,   0.6558,  -4.9084,  -2.2768,\n",
       "          -8.7956,  -3.8150,  -4.7625],\n",
       "        [  1.4345,  -6.4914,   4.3994,  -4.8085,   0.4768,  -5.0231,   3.5852,\n",
       "          -5.5745,  -5.0034,  -5.3485],\n",
       "        [ -0.5615,  -4.4639,   2.9621,  -3.2637,   3.8469,  -3.9994,   1.1531,\n",
       "          -2.0112,  -3.4990,  -3.0156],\n",
       "        [ -0.6535,  -4.8460,  -4.5301,  -2.7064,  -2.4342,  -1.5607,  -0.8436,\n",
       "          -1.6060,   8.5437,  -2.3402],\n",
       "        [  8.1329,  -5.8284,   0.7923,   2.0105,  -2.8794,  -5.9066,   3.7662,\n",
       "          -6.4960,  -6.8912,  -8.1103],\n",
       "        [ -0.0744,  -6.7470,   5.4546,  -5.0159,   1.2831,  -5.8522,   1.3562,\n",
       "          -4.5867,  -4.5867,  -5.3888],\n",
       "        [ -1.9762,  -2.5750,  -3.2638,  -2.1379,  -2.8180,   5.4063,  -2.6843,\n",
       "           2.7512,   0.4220,   1.1652],\n",
       "        [ -2.9062,  -6.4217,  -4.1527,  -5.3229,  -6.6249,   1.1545,  -3.8482,\n",
       "           9.0253,  -1.0553,   1.4258],\n",
       "        [ -3.8390,  -2.0754,  -3.5045,  -1.0457,  -3.6357,   6.0470,  -4.6301,\n",
       "          -0.2679,  -3.7264,   4.1532],\n",
       "        [ -0.9573,  15.4528,  -2.8595,   1.5541,   2.2115,  -5.7967,  -2.1247,\n",
       "          -9.4230,  -2.9447,  -5.9436],\n",
       "        [  0.2502,  -5.6298,   3.4228,  -3.8866,   0.0629,  -6.1239,   3.7848,\n",
       "          -5.3567,  -3.9772,  -6.3805],\n",
       "        [ -2.3151,  -8.4160,   2.1028,  -6.3085,   4.1469,  -6.0266,   4.9866,\n",
       "          -6.2618,  -4.0174,  -7.9025],\n",
       "        [  4.6514,  -4.5691,  -0.1509,   0.5898,  -2.1177,  -5.6628,   2.7604,\n",
       "          -7.2307,  -3.7754,  -6.0466],\n",
       "        [ -2.5202,  -3.9490,  -4.1884,  -4.0372,  -2.9075,   0.2441,  -3.5754,\n",
       "           2.8569,  -3.3291,  10.1565],\n",
       "        [ -2.2079,  -4.2840,  -0.6255,   5.1656,   2.6244,  -6.3968,   0.9950,\n",
       "          -5.6805,  -2.8601,  -5.7606],\n",
       "        [ -1.0705,  -4.8105,  -1.9659,  -3.8230,  -2.8995,  -3.0959,  -1.8859,\n",
       "          -1.6523,  13.7785,  -3.6342],\n",
       "        [ -0.7207,  -1.1710,  -2.6899,  -2.0194,  -1.5340,  -3.9827,  -0.2663,\n",
       "          -3.5572,  11.9908,  -6.2302],\n",
       "        [ -3.3821,   1.1103,  -3.6362,   7.2293,  -0.7826,  -2.2541,  -2.1370,\n",
       "          -3.9362,  -3.2974,  -3.5806],\n",
       "        [  0.1912,  -4.6133,  -1.1574,   5.1119,  -0.1260,  -6.3865,   0.9033,\n",
       "          -5.6536,  -0.5017,  -8.1053],\n",
       "        [ -1.4506,  -3.9487,  -3.1425,  -2.0542,  -0.8913,  -1.9143,  -0.4742,\n",
       "          -2.1405,   9.1858,  -5.1699],\n",
       "        [  8.6771,  -5.8021,   0.7386,   0.2701,  -2.6813,  -5.7268,   2.5230,\n",
       "          -6.2194,  -6.5031,  -7.9695],\n",
       "        [ -3.8508,  -6.5741,  -5.3372,  -6.6107,  -6.3709,   0.5493,  -3.2506,\n",
       "           9.6277,  -1.6208,   1.7356],\n",
       "        [ -1.7437,  -3.5428,  -3.9104,  -3.6168,  -2.2863,   8.3217,  -4.0583,\n",
       "           0.2287,   2.2063,   1.7411],\n",
       "        [ -1.9947,  -7.5650,  -4.5422,  -5.3010,  -6.1919,   0.9368,  -3.3433,\n",
       "           9.1492,  -0.2044,   0.4067],\n",
       "        [ -4.2482,  -3.4116,  -4.4370,  -1.9392,  -3.6451,   1.4403,  -5.7449,\n",
       "           2.6005,  -2.1925,  12.9997],\n",
       "        [  5.5282,  -4.5733,  -0.6905,  -1.7060,  -1.6050,  -3.0999,   3.2854,\n",
       "          -3.9233,  -3.3711,  -5.5356],\n",
       "        [ -0.4630,  16.7904,  -2.0869,   0.6952,   2.8984,  -6.5354,  -2.1379,\n",
       "         -10.1655,  -4.4242,  -5.8514],\n",
       "        [  2.0908,  -0.2849,  -0.7823,   0.7371,  -0.7799,  -4.7103,   1.6000,\n",
       "          -5.7064,  -1.2992,  -4.1229],\n",
       "        [ -2.7507,  -2.6841,  -5.6314,  -5.1321,  -4.4829,   1.0344,  -3.3146,\n",
       "           3.6655,  -2.3252,   5.5886],\n",
       "        [ -1.8442,  -8.6186,   1.2959,  -3.7970,   1.5418,  -6.1932,   6.6666,\n",
       "          -8.3834,  -4.9245,  -7.1637],\n",
       "        [ -3.4386,  -4.6499,  -2.8221,  -8.0262,  -4.9135,  -1.7770,  -4.0795,\n",
       "           6.4863,   0.8609,   3.3483],\n",
       "        [ -0.8520,  -4.8927,   5.0333,  -2.7989,   1.7794,  -4.9703,   2.8479,\n",
       "          -4.9823,  -2.9289,  -5.1368],\n",
       "        [ -1.2614,  12.5156,  -2.5803,  -0.2261,   1.0924,  -2.5869,  -1.8954,\n",
       "          -6.5106,  -1.8858,  -3.3373],\n",
       "        [ -1.8175,  -5.6620,   5.3267,  -5.4565,   2.4416,  -7.3994,   1.7202,\n",
       "          -5.3297,  -4.8868,  -6.6875],\n",
       "        [ -0.0770,  -5.4694,   5.2623,  -4.2217,   0.3283,  -6.2894,   3.7772,\n",
       "          -6.5329,  -4.4395,  -8.1966],\n",
       "        [ -3.3881,  -7.1805,   1.3044,  -5.8154,   6.1394,  -4.1066,   3.2660,\n",
       "          -4.4442,  -2.5120,  -5.6580],\n",
       "        [ -3.4641,  -5.4176,   2.7376,  -6.6380,   4.3593,  -6.4034,   2.0570,\n",
       "          -5.8996,  -2.9915,  -6.8605],\n",
       "        [ -2.2598,  -3.1614,  -4.3028,  -3.0180,  -1.5836,   8.3186,  -3.3009,\n",
       "          -0.1212,   2.7211,   1.9714],\n",
       "        [ -0.2867,  -1.1773,  -2.1651,  -3.6656,  -3.4014,  -2.1470,  -0.6943,\n",
       "          -2.8781,   8.4226,  -1.2311],\n",
       "        [ -3.6006,  -6.1950,   6.0312,  -3.9610,   4.3058,  -7.5865,  -0.9201,\n",
       "          -6.3533,  -4.7261,  -6.8009],\n",
       "        [  0.0465,  -5.6296,   5.9025,  -5.7156,   1.8854,  -5.5877,   3.2886,\n",
       "          -4.4908,  -4.5974,  -7.4573],\n",
       "        [ -0.2238,  -4.3831,  -3.9338,  -2.9769,  -2.1027,  -2.3082,  -1.0531,\n",
       "          -1.9367,  10.3324,  -3.9456],\n",
       "        [ -2.2604,  -8.2720,   4.3410,  -5.3905,   5.4341,  -7.3273,   1.8785,\n",
       "          -6.9779,  -5.3196,  -8.0978],\n",
       "        [ -0.5597,  -4.2009,  -4.8645,  -2.1438,  -2.3717,  -1.3520,  -0.5173,\n",
       "          -1.8786,   8.2393,  -2.3725],\n",
       "        [  8.7638,  -5.5533,   1.2652,  -0.8690,  -2.8690,  -5.5264,   3.1054,\n",
       "          -5.9279,  -5.6877,  -8.2155],\n",
       "        [ -4.0426,  -7.0060,  -5.6537,  -6.9413,  -6.5838,   0.4218,  -3.5993,\n",
       "           9.2721,  -0.9876,   1.6880],\n",
       "        [ -2.2779,  -5.6627,  -5.5616,  -2.9257,  -5.3781,   1.3430,  -2.5229,\n",
       "           7.9144,   0.7299,  -0.4309],\n",
       "        [ -0.2183,  -6.1469,  -4.0250,  -2.9781,  -2.2049,  -2.0105,  -1.4139,\n",
       "          -0.5163,  11.2968,  -3.1459],\n",
       "        [ -2.2968,  -2.8938,  -3.6736,  -4.8320,  -1.1164,   8.8176,  -3.4414,\n",
       "           0.2373,   0.8404,   1.1646],\n",
       "        [ -1.5279,  12.2291,  -0.5196,  -0.1522,   3.8647,  -2.8328,  -1.7706,\n",
       "          -7.0839,  -1.7264,  -3.9983],\n",
       "        [ -1.6909,  15.7029,  -3.0353,   1.0390,   1.4585,  -4.9977,  -2.1949,\n",
       "          -8.9128,  -2.7248,  -5.3634],\n",
       "        [ -1.7509,  -0.5724,  -0.4549,   3.8779,   0.8910,  -7.7370,  -0.4885,\n",
       "          -6.6437,  -2.5027,  -6.5358],\n",
       "        [ -3.4940,  -1.7095,   0.6459,   7.3119,   0.6492,  -9.9650,   0.8052,\n",
       "          -9.1880,  -2.3130, -10.3229],\n",
       "        [ -2.0678,  -4.4686,  -3.6054,  -7.6792,  -6.6231,   1.8375,  -4.8578,\n",
       "           7.7938,  -2.4413,   4.5022],\n",
       "        [ -1.2071,  -5.3047,  -2.2772,  -3.2246,  -2.9024,  -2.4180,  -1.2969,\n",
       "          -0.6167,  13.2837,  -3.1873],\n",
       "        [ -3.9779,  -6.0644,  -5.1923,  -6.4384,  -6.0822,   0.4809,  -3.1845,\n",
       "           9.4559,  -1.3202,   1.6930],\n",
       "        [  6.3351,  -4.9033,   0.3427,  -0.4023,  -3.4249,  -4.2813,   3.9997,\n",
       "          -5.6586,  -4.8419,  -6.2716],\n",
       "        [  1.4353,  -4.7578,   4.2903,  -4.7143,  -0.2241,  -6.5604,   2.2063,\n",
       "          -3.9223,  -1.0041,  -6.2584],\n",
       "        [  1.6604,  -2.6565,   0.1690,   1.5939,  -0.4442,  -4.9228,   2.9858,\n",
       "          -6.3150,  -1.9309,  -5.9708],\n",
       "        [ -3.5665,  -8.0839,   5.7731,  -3.6038,   5.5334,  -8.2582,  -0.7033,\n",
       "          -7.0293,  -5.0486,  -7.9841],\n",
       "        [  0.0683,  -2.6199,  -1.3603,   4.9335,  -1.1936,  -3.2144,  -0.5242,\n",
       "          -4.8812,  -2.5544,  -4.2866],\n",
       "        [ -1.3429,  15.8493,  -3.0740,  -0.0236,   2.0051,  -3.2507,  -1.9763,\n",
       "          -6.9997,  -2.1826,  -3.7941],\n",
       "        [  1.0404,  -7.0773,   8.3134,  -6.1856,   1.1458,  -6.6126,   0.9364,\n",
       "          -4.8526,  -5.9252,  -6.3031],\n",
       "        [ -0.1853,  -5.2438,  -4.7956,  -1.6393,  -1.1623,  -3.2519,  -0.3785,\n",
       "          -1.6465,   9.7781,  -3.9370],\n",
       "        [ -3.6701,  -8.5081,   3.2612,  -6.9327,   6.3494,  -7.4019,   2.9157,\n",
       "          -6.1329,  -3.6250,  -7.8411],\n",
       "        [  0.0225,  12.1635,  -1.6538,   0.6861,   1.9170,  -2.9246,  -2.0544,\n",
       "          -6.7826,  -1.6190,  -4.4497],\n",
       "        [ -1.6140,  -4.1482,  -2.0876,  -3.8943,  -1.8228,  -3.1841,  -2.1376,\n",
       "          -0.7971,  13.8608,  -3.5127],\n",
       "        [ -3.4352,  -3.5204,  -4.7770,  -3.2941,  -0.6103,  10.0996,  -3.7071,\n",
       "          -1.0586,  -3.4838,   2.0524],\n",
       "        [ -3.0957,  -3.4922,  -3.5146,  -2.6499,  -3.7369,   1.4859,  -5.0963,\n",
       "           2.3400,  -1.1239,  11.9793],\n",
       "        [ -4.1406,  -3.5366,  -5.2388,  -6.4340,  -1.5201,  11.7597,  -4.6716,\n",
       "           0.0682,   0.4855,   1.5252],\n",
       "        [  6.8756,  -4.9981,   0.0316,  -1.0757,  -1.8022,  -4.1868,   2.5509,\n",
       "          -4.3294,  -4.1811,  -6.2740],\n",
       "        [  0.7455,  -1.9677,  -3.0916,   5.9508,  -0.7718,  -6.0874,  -1.1373,\n",
       "          -5.9475,  -3.3657,  -5.8746],\n",
       "        [ -1.4199,  -2.3340,   6.7742,  -2.9260,   1.5458,  -5.8330,  -1.2578,\n",
       "          -3.9584,  -4.7105,  -4.3023],\n",
       "        [  9.5095,  -4.9026,   1.3679,   1.3027,  -2.8216,  -6.1346,   2.2952,\n",
       "          -6.6693,  -6.9703,  -8.2638],\n",
       "        [  1.4198,  -4.0492,   4.1068,  -1.9951,   0.3635,  -5.1792,   2.0003,\n",
       "          -4.5597,  -2.0436,  -6.9081],\n",
       "        [ -3.8017,  -2.7340,  -5.4572,  -5.5957,  -1.9614,  12.6862,  -5.0835,\n",
       "           1.8689,  -0.0467,   2.8595],\n",
       "        [  0.7940,  -1.0199,  -1.9299,   5.7988,  -2.6008,  -3.6529,  -1.3731,\n",
       "          -5.0391,  -1.9534,  -4.6668],\n",
       "        [ -1.3162,  -9.1588,  -0.0855,  -3.6329,   2.4840,  -6.5441,   7.8178,\n",
       "          -8.6299,  -4.4685,  -7.5742],\n",
       "        [ -3.7535,  -6.8371,  -5.9325,  -6.3887,  -5.7889,   0.0333,  -2.8976,\n",
       "           9.0922,  -1.2643,   1.1351],\n",
       "        [ -0.8467,  16.3389,  -2.9742,   1.2133,   2.3808,  -5.7448,  -1.8229,\n",
       "          -9.1378,  -3.2858,  -5.7979],\n",
       "        [ -0.8197,  -2.9984,  -3.0889,  -2.4184,  -2.4149,  -1.6098,  -1.6527,\n",
       "          -1.2254,   5.3984,  -2.7371],\n",
       "        [  6.8094,  -6.1896,  -0.6615,   1.5485,  -2.7705,  -6.3532,   3.8619,\n",
       "          -7.0542,  -6.5328,  -8.6268],\n",
       "        [ -1.3951,  12.8261,  -2.5569,   2.1193,  -0.4951,  -4.6386,  -2.6352,\n",
       "          -7.9493,  -3.0083,  -4.4782],\n",
       "        [ -1.8731,  -3.9079,   2.8031,  -3.2634,   4.6765,  -6.3318,   2.2921,\n",
       "          -5.2141,  -1.8028,  -5.4172],\n",
       "        [ -0.0733,  -7.9669,   6.2714,  -6.0835,   1.4935,  -5.7976,   3.0272,\n",
       "          -5.1039,  -5.5651,  -7.0340]], device='cuda:0',\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_output = model(test_images.view(-1, 28, 28))\n",
    "test_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 2, 1, 1, 6, 1, 4, 6, 5, 7, 4, 5, 7, 3, 4, 1, 2, 4, 8, 0, 2, 5, 7, 9,\n",
       "        1, 4, 6, 0, 9, 3, 8, 8, 3, 3, 8, 0, 7, 5, 7, 9, 6, 1, 3, 7, 6, 7, 2, 1,\n",
       "        2, 2, 4, 4, 5, 8, 2, 2, 8, 4, 8, 0, 7, 7, 8, 5, 1, 1, 2, 3, 9, 8, 7, 0,\n",
       "        2, 6, 2, 3, 1, 2, 8, 4, 1, 8, 5, 9, 5, 0, 3, 2, 0, 6, 5, 3, 6, 7, 1, 8,\n",
       "        0, 1, 4, 2])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = torch.max(test_output, 1)[1]\n",
    "predicted\n",
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage correct: 0.92%\n"
     ]
    }
   ],
   "source": [
    "# percentage_correct = 0 + 1 if prediced[i] == test_labels[i] for i in range(100)\n",
    "\n",
    "correct = [1 for i in range(100) if predicted[i] == test_labels[i]]\n",
    "percentage_correct = sum(correct)/100\n",
    "print(f\"percentage correct: {percentage_correct}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, dataloader):\n",
    "    accuracy = 0\n",
    "    with torch.no_grad():\n",
    "        for test_images, test_labels in dataloader:\n",
    "            test_images = test_images.to('cuda')\n",
    "            #we send it to the model for inference\n",
    "            test_output = model(test_images.view(-1, 28, 28))\n",
    "\n",
    "            predicted = torch.max(test_output, 1)[1]\n",
    "            num_correct = [1 for i in range(batchsize) if predicted[i] == test_labels[i]]\n",
    "            accuracy += sum(num_correct)\n",
    "    num_batches = len(dataloader)\n",
    "    size = len(dataloader.dataset)\n",
    "    accuracy = accuracy / size\n",
    "    \n",
    "    print(f\"total accuracy of model: {100*accuracy}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total accuracy of model: 88.49000000000001%\n"
     ]
    }
   ],
   "source": [
    "test(model, test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
